{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiass data indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aoai_connection_name = \"admro-m6gjoqlt-switzerlandnorth.\"\n",
    "data_set_name = \"raw_data_ds\"\n",
    "asset_name = \"qknows_aoai_fiass_mlindex\"\n",
    "doc_intelligence_connection_name = \"beyondtext-doc-intelligence\"\n",
    "vision_deploy_name = \"gpt-4\"\n",
    "aoai_embedding_model_name = \"text-embedding-ada-002\"\n",
    "\n",
    "\n",
    "experiment_name = \"fiass-embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../chunk_caption_component/')\n",
    "sys.path.append('../enhanced_doc_analyzer_component/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure.ai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example of registering the component in a workspace\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLClient\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultAzureCredential\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Get workspace\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'azure.ai'"
     ]
    }
   ],
   "source": [
    "# Example of registering the component in a workspace\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Get workspace\n",
    "ml_client = MLClient.from_config(\n",
    "    credential=DefaultAzureCredential()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azure.ai.ml import load_component, load_environment\n",
    "\n",
    "ml_registry = MLClient(credential=DefaultAzureCredential(), registry_name=\"azureml\")\n",
    "\n",
    "# Reads input folder of files containing chunks and their metadata as batches, in parallel, and generates embeddings for each chunk. Output format is produced and loaded by `azureml.rag.embeddings.EmbeddingContainer`.\n",
    "generate_embeddings_component = ml_registry.components.get(\n",
    "    \"llm_rag_generate_embeddings\", label=\"latest\"\n",
    ")\n",
    "# Reads input folder produced by `azureml.rag.embeddings.EmbeddingsContainer.save()` and inserts all documents (chunk, metadata, embedding_vector) int a Faiss index and in-memory document store. Writes an MLIndex yaml detailing the index and embeddings model information.\n",
    "create_faiss_index_component = ml_registry.components.get(\n",
    "    \"llm_rag_create_faiss_index\", label=\"latest\"\n",
    ")\n",
    "# Takes a uri to a storage location where an MLIndex yaml is stored and registers it as an MLIndex Data asset in the AzureML Workspace.\n",
    "register_mlindex_component = ml_registry.components.get(\n",
    "    \"llm_rag_register_mlindex_asset\", label=\"latest\"\n",
    ")\n",
    "\n",
    "# Load components and environment\n",
    "analyzer_component = load_component(source=\"./enhanced_doc_analyzer_component/doc_analyzer_component.yaml\")\n",
    "chunk_caption_index = load_component(source=\"./chuck_caption_component/chuck_caption_component.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azureml.rag.utils.deployment import infer_deployment\n",
    "from azureml.rag.utils.connections import get_connection_by_id_v2\n",
    "\n",
    "aoai_connection_id = ml_client.connections.get(aoai_connection_name).id\n",
    "aoai_connection = get_connection_by_id_v2(aoai_connection_id)\n",
    "\n",
    "\n",
    "\n",
    "embeddings_model_uri = f\"azure_open_ai://deployment/{aoai_embedding_model_name}/model/{aoai_embedding_model_name}\"\n",
    "# embeddings_model = \"hugging_face://model/sentence-transformers/all-mpnet-base-v2\"\n",
    "embeddings_model = embeddings_model_uri\n",
    "\n",
    "\n",
    "doc_intelligence_connection = ml_client.connections.get(doc_intelligence_connection_name)\n",
    "\n",
    "# Get the data asset with version\n",
    "raw_papers = ml_client.data.get(data_set_name, version=\"1\")\n",
    "# Create Input object for the data\n",
    "pdf_input = Input(type=AssetTypes.URI_FOLDER, path=raw_papers.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import dsl, Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml import Input, Output\n",
    "from azure.ai.ml.entities._job.pipeline._io import PipelineInput\n",
    "from typing import Optional\n",
    "\n",
    "def optional_pipeline_input_provided(input: Optional[PipelineInput]):\n",
    "    \"\"\"Checks if optional pipeline inputs are provided.\"\"\"\n",
    "    return input is not None and input._data is not None\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    description=\"Combined document analysis and indexing pipeline\",\n",
    "    default_compute=\"gpu-cluster\"\n",
    ")\n",
    "def document_processing_pipeline(\n",
    "    # Document Analyzer inputs\n",
    "    \n",
    "    pdf_folder,\n",
    "    asset_name: str,\n",
    "    doc_intel_connection_id: str,\n",
    "    confidence_threshold: float = 0.5,\n",
    "    min_length: int = 10,\n",
    "    overlap_threshold: float = 0.5,\n",
    "    ignore_roles: str = \"pageFooter,footnote,pageHeader\",\n",
    "    vision_deployment_name: str = \"gpt-4\",\n",
    "    embeddings_model: str = \"hugging_face://model/sentence-transformers/all-mpnet-base-v2\",\n",
    "    embeddings_container=None,\n",
    "    aoai_connection_id: str = None,\n",
    "    # Compute settings\n",
    "    analyzer_compute: str = \"gpu-cluster\",\n",
    "    indexer_compute: str = \"cpu-cluster\"\n",
    "\n",
    "):\n",
    "    # Document Analyzer step\n",
    "    analysis_job = analyzer_component(\n",
    "        input_folder=pdf_folder,\n",
    "        doc_intel_connection_id=doc_intel_connection_id,\n",
    "        confidence_threshold=confidence_threshold,\n",
    "        min_length=min_length,\n",
    "        overlap_threshold=overlap_threshold,\n",
    "        ignore_roles=ignore_roles\n",
    "    )\n",
    "    analysis_job.compute = analyzer_compute\n",
    "\n",
    "    # Chunk Caption Index step\n",
    "    # Using the output from document analyzer as input\n",
    "    chunk_caption_job = chunk_caption_index(\n",
    "        input_folder=analysis_job.outputs.output_dir,\n",
    "        azure_openai_connection_id=aoai_connection_id,\n",
    "        vision_deployment_name=vision_deployment_name,\n",
    "    )\n",
    "    chunk_caption_job.compute = indexer_compute\n",
    "\n",
    "    generate_embeddings = generate_embeddings_component(\n",
    "        chunks_source=chunk_caption_job.outputs.output_folder,\n",
    "        embeddings_container=embeddings_container,\n",
    "        embeddings_model=embeddings_model,\n",
    "    )\n",
    "    if optional_pipeline_input_provided(aoai_connection_id):\n",
    "        generate_embeddings.environment_variables[\n",
    "            \"AZUREML_WORKSPACE_CONNECTION_ID_AOAI\"\n",
    "        ] = aoai_connection_id\n",
    "    if optional_pipeline_input_provided(embeddings_container):\n",
    "        # If provided, `embeddings_container` is expected to be a URI to folder, the folder can be empty.\n",
    "        # Each sub-folder is generated by a `create_embeddings_component` run and can be reused for subsequent embeddings runs.\n",
    "        generate_embeddings.outputs.embeddings = Output(\n",
    "            type=\"uri_folder\", path=f\"{embeddings_container.path}/{{name}}\"\n",
    "        )\n",
    "    generate_embeddings.compute = indexer_compute\n",
    "\n",
    "    create_faiss_index = create_faiss_index_component(\n",
    "        embeddings=generate_embeddings.outputs.embeddings,\n",
    "    )\n",
    "    create_faiss_index.compute = indexer_compute\n",
    "\n",
    "    register_mlindex = register_mlindex_component(\n",
    "        storage_uri=create_faiss_index.outputs.index, asset_name=asset_name\n",
    "    )\n",
    "    register_mlindex.compute = indexer_compute\n",
    "    return {\n",
    "        \"mlindex_asset_uri\": create_faiss_index.outputs.index,\n",
    "        \"mlindex_asset_id\": register_mlindex.outputs.asset_id,\n",
    "        \"analyzer_output\": analysis_job.outputs.output_dir,\n",
    "        \"final_output\": chunk_caption_job.outputs.output_folder\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipeline = document_processing_pipeline(\n",
    "    # Document Analyzer params\n",
    "    pdf_folder=pdf_input,\n",
    "    asset_name=asset_name,\n",
    "    doc_intel_connection_id=doc_intelligence_connection.id,\n",
    "    confidence_threshold=0.3,\n",
    "    min_length=15,\n",
    "    overlap_threshold=0.7,\n",
    "    ignore_roles=\"pageFooter,footnote,pageHeader\",\n",
    "    \n",
    "    # Chunk Caption Index params\n",
    "    aoai_connection_id=aoai_connection_id,\n",
    "    vision_deployment_name=vision_deploy_name,\n",
    "    embeddings_model=embeddings_model,\n",
    "    embeddings_container=None,\n",
    "    \n",
    "    # Compute settings\n",
    "    analyzer_compute=\"gpu-cluster\",\n",
    "    indexer_compute=\"cpu-cluster\"\n",
    ")\n",
    "\n",
    "# These are added so that in progress index generations can be listed in UI, this tagging is done automatically by UI.\n",
    "pipeline.properties[\"azureml.mlIndexAssetName\"] = asset_name\n",
    "pipeline.properties[\"azureml.mlIndexAssetKind\"] = \"faiss\"\n",
    "pipeline.properties[\"azureml.mlIndexAssetSource\"] = \"raw_papers\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Submit the pipeline\n",
    "run = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    experiment_name=experiment_name,\n",
    "    tags={\"type\": \"document-processing\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.rag.mlindex import MLIndex\n",
    "\n",
    "\n",
    "\n",
    "question = \"how many steps are in metalloporphyrins synthesis?\"\n",
    "\n",
    "faias_retriever = MLIndex(\n",
    "    ml_client.data.get(asset_name, label=\"latest\")\n",
    ").as_langchain_retriever()\n",
    "retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.rag.utils.connections import get_connection_by_id_v2\n",
    "\n",
    "aoai_connection_id = ml_client.connections.get(\"aoai-sweden-505\").id\n",
    "aoai_connection = get_connection_by_id_v2(aoai_connection_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from azureml.rag.models import init_llm, parse_model_uri\n",
    "\n",
    "model_config = parse_model_uri(\n",
    "    \"azure_open_ai://deployment/gpt-35-turbo/model/gpt-35-turbo\"\n",
    ")\n",
    "model_config[\"azure_endpoint\"] = aoai_connection.target\n",
    "model_config[\"api_key\"] = aoai_connection.api_key\n",
    "model_config[\"temperature\"] = 0.3\n",
    "model_config[\"max_retries\"] = 3\n",
    "model_config[\"deployment\"] = \"gpt-4v\"\n",
    "model_config[\"model\"] = \"gpt-4\"\n",
    "\n",
    "llm=init_llm(model_config)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "qa.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_from_llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever_from_llm\n",
    ")\n",
    "\n",
    "qa.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
