{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AZURE AI Search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aoai_connection_name = \"open_ai_connection\"\n",
    "aoai_connection_name_emb = \"open_ai_connection\"\n",
    "acs_connection_name = \"acs-connection\"\n",
    "# data_set_name = \"papers\"\n",
    "data_set_name = \"row_data_ds\"\n",
    "asset_name = \"aoai_acs_mlindex\"\n",
    "doc_intelligence_connection_name = \"doc-intelligence-connection\"\n",
    "# data_set_name = \"sample-data\"\n",
    "asset_name = f\"{data_set_name}_aoai_acs_emb3_large_mlindex_v2_clean\"\n",
    "vision_deploy_name = \"gpt-4o\"\n",
    "# aoai_embedding_model_name = \"text-embedding-ada-002\"\n",
    "aoai_embedding_model_name = \"text-embedding-3-large\"\n",
    "\n",
    "acs_config = {\n",
    "    \"index_name\": asset_name,\n",
    "}\n",
    "\n",
    "experiment_name = f\"{data_set_name}-acs-embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../chunk_caption_component/')\n",
    "sys.path.append('../enhanced_doc_analyzer_component/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "# Example of registering the component in a workspace\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Get workspace\n",
    "ml_client = MLClient.from_config(\n",
    "    credential=DefaultAzureCredential()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from azure.ai.ml import load_component, load_environment\n",
    "\n",
    "ml_registry = MLClient(credential=DefaultAzureCredential(), registry_name=\"azureml\")\n",
    "\n",
    "# Reads input folder of files containing chunks and their metadata as batches, in parallel, and generates embeddings for each chunk. Output format is produced and loaded by `azureml.rag.embeddings.EmbeddingContainer`.\n",
    "generate_embeddings_component = ml_registry.components.get(\n",
    "    \"llm_rag_generate_embeddings\", label=\"latest\"\n",
    ")\n",
    "# Reads an input folder produced by `azureml.rag.embeddings.EmbeddingsContainer.save()` and pushes all documents (chunk, metadata, embedding_vector) into an Azure Cognitive Search index. Writes an MLIndex yaml detailing the index and embeddings model information.\n",
    "update_acs_index_component = ml_registry.components.get(\n",
    "    \"llm_rag_update_acs_index\", label=\"latest\"\n",
    ")\n",
    "# Takes a uri to a storage location where an MLIndex yaml is stored and registers it as an MLIndex Data asset in the AzureML Workspace.\n",
    "register_mlindex_component = ml_registry.components.get(\n",
    "    \"llm_rag_register_mlindex_asset\", label=\"latest\"\n",
    ")\n",
    "\n",
    "# Load components and environment\n",
    "analyzer_component = load_component(source=\"./enhanced_doc_analyzer_component/doc_analyzer_component.yaml\")\n",
    "chunk_caption_index = load_component(source=\"./chuck_caption_component/chuck_caption_component.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploads `embeddings` into Azure Cognitive Search instance specified in `acs_config`. The Index will be created if it doesn't exist.\n",
      "\n",
      "The Index will have the following fields populated:\n",
      "- \"id\", String, key=True\n",
      "- \"content\", String\n",
      "- \"contentVector\", Collection(Single)\n",
      "- \"category\", String\n",
      "- \"url\", String\n",
      "- \"filepath\", String\n",
      "- \"content_hash\", String\n",
      "- \"meta_json_string\", String\n",
      "\n",
      "\"meta_json_string\" contains all metadata for a document serialized as a JSON string.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(update_acs_index_component.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from azureml.rag.utils.deployment import infer_deployment\n",
    "from azureml.rag.utils.connections import get_connection_by_id_v2\n",
    "from azure.ai.ml import dsl, Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml import Input, Output\n",
    "from azure.ai.ml.entities._job.pipeline._io import PipelineInput\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "\n",
    "aoai_connection_id = ml_client.connections.get(aoai_connection_name).id\n",
    "aoai_connection_emb_id = ml_client.connections.get(aoai_connection_name_emb).id\n",
    "aoai_connection = get_connection_by_id_v2(aoai_connection_id)\n",
    "aoai_connection_emb = get_connection_by_id_v2(aoai_connection_emb_id)\n",
    "\n",
    "\n",
    "\n",
    "embeddings_model_uri = f\"azure_open_ai://deployment/{aoai_embedding_model_name}/model/{aoai_embedding_model_name}\"\n",
    "# embeddings_model = \"hugging_face://model/sentence-transformers/all-mpnet-base-v2\"\n",
    "embeddings_model = embeddings_model_uri\n",
    "\n",
    "\n",
    "doc_intelligence_connection = ml_client.connections.get(doc_intelligence_connection_name)\n",
    "acs_connection = ml_client.connections.get(acs_connection_name)\n",
    "\n",
    "# Get the data asset with version\n",
    "raw_papers = ml_client.data.get(data_set_name, version=\"1\")\n",
    "# Create Input object for the data\n",
    "pdf_input = Input(type=AssetTypes.URI_FOLDER, path=raw_papers.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def optional_pipeline_input_provided(input: Optional[PipelineInput]):\n",
    "    \"\"\"Checks if optional pipeline inputs are provided.\"\"\"\n",
    "    return input is not None and input._data is not None\n",
    "\n",
    "def use_automatic_compute(component, instance_count=1, instance_type=\"Standard_E8s_v3\"):\n",
    "    \"\"\"Configure input `component` to use automatic compute with `instance_count` and `instance_type`.\n",
    "\n",
    "    This avoids the need to provision a compute cluster to run the component.\n",
    "    \"\"\"\n",
    "    component.set_resources(\n",
    "        instance_count=instance_count,\n",
    "        instance_type=instance_type,\n",
    "        properties={\"compute_specification\": {\"automatic\": True}},\n",
    "    )\n",
    "    return component\n",
    "\n",
    "@dsl.pipeline(\n",
    "    description=\"Combined document analysis and azure AI search indexing pipeline\",\n",
    "    default_compute=\"serverless\"\n",
    ")\n",
    "def document_processing_pipeline(\n",
    "    # Document Analyzer inputs\n",
    "    \n",
    "    pdf_folder,\n",
    "    asset_name: str,\n",
    "    acs_config: str, \n",
    "    acs_connection_id: str,\n",
    "    doc_intel_connection_id: str,\n",
    "    confidence_threshold: float = 0.5,\n",
    "    min_length: int = 10,\n",
    "    overlap_threshold: float = 0.5,\n",
    "    ignore_roles: str = \"pageFooter,footnote,pageHeader\",\n",
    "    vision_deployment_name: str = \"gpt-4o\",\n",
    "    embeddings_model: str = \"hugging_face://model/sentence-transformers/all-mpnet-base-v2\",\n",
    "    embeddings_container=None,\n",
    "    aoai_connection_id: str = None,\n",
    "    aoai_connection_emb_id: str = None,\n",
    "    # Compute settings\n",
    "    analyzer_compute: str = \"gpu-cluster\",\n",
    "    indexer_compute: str = \"cpu-cluster\"\n",
    "\n",
    "):\n",
    "    # Document Analyzer step\n",
    "    analysis_job = analyzer_component(\n",
    "        input_folder=pdf_folder,\n",
    "        doc_intel_connection_id=doc_intel_connection_id,\n",
    "        confidence_threshold=confidence_threshold,\n",
    "        min_length=min_length,\n",
    "        overlap_threshold=overlap_threshold,\n",
    "        ignore_roles=ignore_roles\n",
    "    )\n",
    "    analysis_job.compute = analyzer_compute\n",
    "\n",
    "    # Chunk Caption Index step\n",
    "    # Using the output from document analyzer as input\n",
    "    chunk_caption_job = chunk_caption_index(\n",
    "        input_folder=analysis_job.outputs.output_dir,\n",
    "        azure_openai_connection_id=aoai_connection_id,\n",
    "        vision_deployment_name=vision_deployment_name,\n",
    "    )\n",
    "    chunk_caption_job.compute = indexer_compute\n",
    "\n",
    "    generate_embeddings = generate_embeddings_component(\n",
    "        chunks_source=chunk_caption_job.outputs.output_folder,\n",
    "        embeddings_container=embeddings_container,\n",
    "        embeddings_model=embeddings_model,\n",
    "    )\n",
    "    # use_automatic_compute(generate_embeddings)\n",
    "    generate_embeddings.compute = indexer_compute\n",
    "    if optional_pipeline_input_provided(aoai_connection_emb_id):\n",
    "        generate_embeddings.environment_variables[\n",
    "            \"AZUREML_WORKSPACE_CONNECTION_ID_AOAI\"\n",
    "        ] = aoai_connection_emb_id\n",
    "    if optional_pipeline_input_provided(embeddings_container):\n",
    "        # If provided, `embeddings_container` is expected to be a URI to folder, the folder can be empty.\n",
    "        # Each sub-folder is generated by a `create_embeddings_component` run and can be reused for subsequent embeddings runs.\n",
    "        generate_embeddings.outputs.embeddings = Output(\n",
    "            type=\"uri_folder\", path=f\"{embeddings_container.path}/{{name}}\"\n",
    "        )\n",
    "\n",
    "    # `update_acs_index` takes the Embedded data produced by `generate_embeddings` and pushes it into an Azure Cognitive Search index.\n",
    "    update_acs_index = update_acs_index_component(\n",
    "        embeddings=generate_embeddings.outputs.embeddings, acs_config=acs_config\n",
    "    )\n",
    "    # use_automatic_compute(update_acs_index)\n",
    "    update_acs_index.compute = indexer_compute\n",
    "    if optional_pipeline_input_provided(acs_connection_id):\n",
    "        update_acs_index.environment_variables[\n",
    "            \"AZUREML_WORKSPACE_CONNECTION_ID_ACS\"\n",
    "        ] = acs_connection_id\n",
    "\n",
    "    register_mlindex = register_mlindex_component(\n",
    "        storage_uri=update_acs_index.outputs.index, asset_name=asset_name\n",
    "    )\n",
    "    # use_automatic_compute(register_mlindex)\n",
    "    register_mlindex.compute = indexer_compute\n",
    "    return {\n",
    "        \"mlindex_asset_uri\": update_acs_index.outputs.index,\n",
    "        \"mlindex_asset_id\": register_mlindex.outputs.asset_id,\n",
    "        \"analyzer_output\": analysis_job.outputs.output_dir,\n",
    "        \"final_output\": chunk_caption_job.outputs.output_folder\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipeline = document_processing_pipeline(\n",
    "    # Document Analyzer params\n",
    "    pdf_folder=pdf_input,\n",
    "    asset_name=asset_name,\n",
    "    doc_intel_connection_id=doc_intelligence_connection.id,\n",
    "    acs_config=json.dumps(acs_config),\n",
    "    acs_connection_id=acs_connection.id,\n",
    "    confidence_threshold=0.3,\n",
    "    min_length=15,\n",
    "    overlap_threshold=0.7,\n",
    "    ignore_roles=\"pageFooter,footnote,pageHeader\",\n",
    "    \n",
    "    # Chunk Caption Index params\n",
    "    aoai_connection_id=aoai_connection_id,\n",
    "    aoai_connection_emb_id=aoai_connection_emb_id,\n",
    "    vision_deployment_name=vision_deploy_name,\n",
    "    embeddings_model=embeddings_model,\n",
    "    embeddings_container=None,\n",
    "    \n",
    "    # Compute settings\n",
    "    analyzer_compute=\"gpu-cluster-a100\",\n",
    "    indexer_compute=\"cpu-cluster-low\"\n",
    ")\n",
    "\n",
    "# These are added so that in progress index generations can be listed in UI, this tagging is done automatically by UI.\n",
    "pipeline.properties[\"azureml.mlIndexAssetName\"] = asset_name\n",
    "pipeline.properties[\"azureml.mlIndexAssetKind\"] = \"acs\"\n",
    "pipeline.properties[\"azureml.mlIndexAssetSource\"] = data_set_name\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chuck_caption_component (3.41 MBs): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3412830/3412830 [00:00<00:00, 21169329.42it/s]\n",
      "\n",
      "\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFileJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Submit the pipeline\n",
    "run = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    experiment_name=experiment_name,\n",
    "    tags={\"type\": \"sample-document-processing\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "# Example of registering the component in a workspace\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Get workspace\n",
    "ml_client = MLClient.from_config(\n",
    "    credential=DefaultAzureCredential()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'row_data_ds_aoai_acs_emb3_large_mlindex_v2_clean'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.rag.mlindex import MLIndex\n",
    "\n",
    "# asset_name = \"sample_aoai_acs_mlindex\"\n",
    "# asset_name = \"aoai_faiss_mlindex\"\n",
    "\n",
    "question = \"how many steps are in metalloporphyrins synthesis?\"\n",
    "\n",
    "retriever = MLIndex(\n",
    "    ml_client.data.get(asset_name, label=\"latest\")\n",
    ")\n",
    "# .as_langchain_retriever()\n",
    "# retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "embeddings:\n",
       "  api_base: https://aoai-sweden-505.openai.azure.com\n",
       "  api_type: azure\n",
       "  api_version: '2024-02-01'\n",
       "  batch_size: '16'\n",
       "  connection:\n",
       "    id: /subscriptions/f804f2da-c27b-45ac-bf80-16d4d331776d/resourceGroups/rg-acadocser-dev-04/providers/Microsoft.MachineLearningServices/workspaces/mlw-acadocser-dev-04/connections/open_ai_connection\n",
       "  connection_type: workspace_connection\n",
       "  deployment: text-embedding-3-large\n",
       "  dimension: 3072\n",
       "  file_format_version: '2'\n",
       "  kind: open_ai\n",
       "  model: text-embedding-3-large\n",
       "  schema_version: '2'\n",
       "index:\n",
       "  api_version: 2024-05-01-preview\n",
       "  connection:\n",
       "    id: /subscriptions/f804f2da-c27b-45ac-bf80-16d4d331776d/resourceGroups/rg-acadocser-dev-04/providers/Microsoft.MachineLearningServices/workspaces/mlw-acadocser-dev-04/connections/acs-connection\n",
       "  connection_type: workspace_connection\n",
       "  endpoint: https://aisearch505.search.windows.net\n",
       "  engine: azure-sdk\n",
       "  field_mapping:\n",
       "    content: content\n",
       "    embedding: contentVector\n",
       "    filename: filepath\n",
       "    metadata: meta_json_string\n",
       "    title: title\n",
       "    url: url\n",
       "  index: row_data_ds_aoai_acs_emb3_large_mlindex_v2_clean\n",
       "  kind: acs\n",
       "  semantic_configuration_name: azureml-default"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.rag.utils.connections import get_connection_by_id_v2\n",
    "\n",
    "aoai_connection_id = ml_client.connections.get(\"aoai-sweden-505\").id\n",
    "aoai_connection = get_connection_by_id_v2(aoai_connection_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from azureml.rag.models import init_llm, parse_model_uri\n",
    "\n",
    "model_config = parse_model_uri(\n",
    "    \"azure_open_ai://deployment/gpt-35-turbo/model/gpt-35-turbo\"\n",
    ")\n",
    "model_config[\"azure_endpoint\"] = aoai_connection.target\n",
    "model_config[\"api_key\"] = aoai_connection.api_key\n",
    "model_config[\"temperature\"] = 0.3\n",
    "model_config[\"max_retries\"] = 3\n",
    "model_config[\"deployment\"] = \"gpt-4v\"\n",
    "model_config[\"model\"] = \"gpt-4\"\n",
    "\n",
    "llm=init_llm(model_config)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "qa.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_from_llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever_from_llm\n",
    ")\n",
    "\n",
    "qa.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
