{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../chunk_caption_component/')\n",
    "sys.path.append('../enhanced_doc_analyzer_component/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "# Example of registering the component in a workspace\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Get workspace\n",
    "ml_client = MLClient.from_config(\n",
    "    credential=DefaultAzureCredential()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current LoggerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from azure.ai.ml import load_component, load_environment\n",
    "\n",
    "ml_registry = MLClient(credential=DefaultAzureCredential(), registry_name=\"azureml\")\n",
    "\n",
    "# Reads input folder of files containing chunks and their metadata as batches, in parallel, and generates embeddings for each chunk. Output format is produced and loaded by `azureml.rag.embeddings.EmbeddingContainer`.\n",
    "generate_embeddings_component = ml_registry.components.get(\n",
    "    \"llm_rag_generate_embeddings\", label=\"latest\"\n",
    ")\n",
    "# Reads an input folder produced by `azureml.rag.embeddings.EmbeddingsContainer.save()` and pushes all documents (chunk, metadata, embedding_vector) into an Azure Cognitive Search index. Writes an MLIndex yaml detailing the index and embeddings model information.\n",
    "update_acs_index_component = ml_registry.components.get(\n",
    "    \"llm_rag_update_acs_index\", label=\"latest\"\n",
    ")\n",
    "# Takes a uri to a storage location where an MLIndex yaml is stored and registers it as an MLIndex Data asset in the AzureML Workspace.\n",
    "register_mlindex_component = ml_registry.components.get(\n",
    "    \"llm_rag_register_mlindex_asset\", label=\"latest\"\n",
    ")\n",
    "\n",
    "# Load components and environment\n",
    "analyzer_component = load_component(source=\"./enhanced_doc_analyzer_component/doc_analyzer_component.yaml\")\n",
    "chunk_caption_index = load_component(source=\"./chuck_caption_component/chuck_caption_component.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current LoggerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from azureml.rag.utils.deployment import infer_deployment\n",
    "from azureml.rag.utils.connections import get_connection_by_id_v2\n",
    "\n",
    "aoai_connection_name = \"aoai-sweden-505\"\n",
    "acs_connection_name = \"acs-connection\"\n",
    "data_set_name = \"row_data_ds\"\n",
    "asset_name = \"aoai_acs_mlindex\"\n",
    "doc_intelligence_connection_name = \"doc-intelligence-connection\"\n",
    "vision_deploy_name = \"gpt-4\"\n",
    "aoai_embedding_model_name = \"text-embedding-ada-002\"\n",
    "\n",
    "acs_config = {\n",
    "    \"index_name\": \"qknows-embedding\",\n",
    "}\n",
    "\n",
    "experiment_name = \"sample-acs-embedding\"\n",
    "\n",
    "aoai_connection_id = ml_client.connections.get(aoai_connection_name).id\n",
    "aoai_connection = get_connection_by_id_v2(aoai_connection_id)\n",
    "\n",
    "\n",
    "\n",
    "embeddings_model_uri = f\"azure_open_ai://deployment/{aoai_embedding_model_name}/model/{aoai_embedding_model_name}\"\n",
    "# embeddings_model = \"hugging_face://model/sentence-transformers/all-mpnet-base-v2\"\n",
    "embeddings_model = embeddings_model_uri\n",
    "\n",
    "\n",
    "doc_intelligence_connection = ml_client.connections.get(doc_intelligence_connection_name)\n",
    "acs_connection = ml_client.connections.get(acs_connection_name)\n",
    "\n",
    "# Get the data asset with version\n",
    "raw_papers = ml_client.data.get(data_set_name, version=\"1\")\n",
    "# Create Input object for the data\n",
    "pdf_input = Input(type=AssetTypes.URI_FOLDER, path=raw_papers.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import dsl, Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml import Input, Output\n",
    "from azure.ai.ml.entities._job.pipeline._io import PipelineInput\n",
    "from typing import Optional\n",
    "import json\n",
    "import os\n",
    "\n",
    "def optional_pipeline_input_provided(input: Optional[PipelineInput]):\n",
    "    \"\"\"Checks if optional pipeline inputs are provided.\"\"\"\n",
    "    return input is not None and input._data is not None\n",
    "\n",
    "def use_automatic_compute(component, instance_count=1, instance_type=\"Standard_E8s_v3\"):\n",
    "    \"\"\"Configure input `component` to use automatic compute with `instance_count` and `instance_type`.\n",
    "\n",
    "    This avoids the need to provision a compute cluster to run the component.\n",
    "    \"\"\"\n",
    "    component.set_resources(\n",
    "        instance_count=instance_count,\n",
    "        instance_type=instance_type,\n",
    "        properties={\"compute_specification\": {\"automatic\": True}},\n",
    "    )\n",
    "    return component\n",
    "\n",
    "@dsl.pipeline(\n",
    "    description=\"Combined document analysis and azure AI search indexing pipeline\",\n",
    "    default_compute=\"serverless\",\n",
    ")\n",
    "def document_processing_pipeline(\n",
    "    # Document Analyzer inputs\n",
    "    \n",
    "    pdf_folder,\n",
    "    asset_name: str,\n",
    "    doc_intel_connection_id: str,\n",
    "    acs_config: str,\n",
    "    acs_connection_id: str,\n",
    "    confidence_threshold: float = 0.5,\n",
    "    min_length: int = 10,\n",
    "    overlap_threshold: float = 0.5,\n",
    "    ignore_roles: str = \"pageFooter,footnote,pageHeader\",\n",
    "    vision_deployment_name: str = \"gpt-4\",\n",
    "    embeddings_model: str = \"hugging_face://model/sentence-transformers/all-mpnet-base-v2\",\n",
    "    embeddings_container=None,\n",
    "    aoai_connection_id: str = None,\n",
    "    # Compute settings\n",
    "    analyzer_compute: str = \"gpu-cluster\",\n",
    "    indexer_compute: str = \"cpu-cluster\"\n",
    "\n",
    "):\n",
    "    # Document Analyzer step\n",
    "    analysis_job = analyzer_component(\n",
    "        input_folder=pdf_folder,\n",
    "        doc_intel_connection_id=doc_intel_connection_id,\n",
    "        confidence_threshold=confidence_threshold,\n",
    "        min_length=min_length,\n",
    "        overlap_threshold=overlap_threshold,\n",
    "        ignore_roles=ignore_roles\n",
    "    )\n",
    "    analysis_job.compute = analyzer_compute\n",
    "\n",
    "    # Chunk Caption Index step\n",
    "    # Using the output from document analyzer as input\n",
    "    chunk_caption_job = chunk_caption_index(\n",
    "        input_folder=analysis_job.outputs.output_dir,\n",
    "        azure_openai_connection_id=aoai_connection_id,\n",
    "        vision_deployment_name=vision_deployment_name,\n",
    "    )\n",
    "    chunk_caption_job.compute = indexer_compute\n",
    "\n",
    "    generate_embeddings = generate_embeddings_component(\n",
    "        chunks_source=chunk_caption_job.outputs.output_folder,\n",
    "        embeddings_container=embeddings_container,\n",
    "        embeddings_model=embeddings_model,\n",
    "    )\n",
    "    use_automatic_compute(generate_embeddings)\n",
    "    if optional_pipeline_input_provided(aoai_connection_id):\n",
    "        generate_embeddings.environment_variables[\n",
    "            \"AZUREML_WORKSPACE_CONNECTION_ID_AOAI\"\n",
    "        ] = aoai_connection_id\n",
    "    if optional_pipeline_input_provided(embeddings_container):\n",
    "        # If provided, `embeddings_container` is expected to be a URI to folder, the folder can be empty.\n",
    "        # Each sub-folder is generated by a `create_embeddings_component` run and can be reused for subsequent embeddings runs.\n",
    "        generate_embeddings.outputs.embeddings = Output(\n",
    "            type=\"uri_folder\", path=f\"{embeddings_container.path}/{{name}}\"\n",
    "        )\n",
    "\n",
    "    # `update_acs_index` takes the Embedded data produced by `generate_embeddings` and pushes it into an Azure Cognitive Search index.\n",
    "    update_acs_index = update_acs_index_component(\n",
    "        embeddings=generate_embeddings.outputs.embeddings, acs_config=acs_config\n",
    "    )\n",
    "    use_automatic_compute(update_acs_index)\n",
    "    if optional_pipeline_input_provided(acs_connection_id):\n",
    "        update_acs_index.environment_variables[\n",
    "            \"AZUREML_WORKSPACE_CONNECTION_ID_ACS\"\n",
    "        ] = acs_connection_id\n",
    "\n",
    "    register_mlindex = register_mlindex_component(\n",
    "        storage_uri=update_acs_index.outputs.index, asset_name=asset_name\n",
    "    )\n",
    "    use_automatic_compute(register_mlindex)\n",
    "    return {\n",
    "        \"mlindex_asset_uri\": update_acs_index.outputs.index,\n",
    "        \"mlindex_asset_id\": register_mlindex.outputs.asset_id,\n",
    "        \"analyzer_output\": analysis_job.outputs.output_dir,\n",
    "        \"final_output\": chunk_caption_job.outputs.output_folder\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipeline = document_processing_pipeline(\n",
    "    # Document Analyzer params\n",
    "    pdf_folder=pdf_input,\n",
    "    asset_name=asset_name,\n",
    "    doc_intel_connection_id=doc_intelligence_connection.id,\n",
    "    acs_config=json.dumps(acs_config),\n",
    "    acs_connection_id=acs_connection.id,\n",
    "    confidence_threshold=0.3,\n",
    "    min_length=15,\n",
    "    overlap_threshold=0.7,\n",
    "    ignore_roles=\"pageFooter,footnote,pageHeader\",\n",
    "    \n",
    "    # Chunk Caption Index params\n",
    "    aoai_connection_id=aoai_connection_id,\n",
    "    vision_deployment_name=vision_deploy_name,\n",
    "    embeddings_model=embeddings_model,\n",
    "    embeddings_container=None,\n",
    "    \n",
    "    # Compute settings\n",
    "    analyzer_compute=\"gpu-cluster\",\n",
    "    indexer_compute=\"cpu-cluster\"\n",
    ")\n",
    "\n",
    "# These are added so that in progress index generations can be listed in UI, this tagging is done automatically by UI.\n",
    "pipeline.properties[\"azureml.mlIndexAssetName\"] = asset_name\n",
    "pipeline.properties[\"azureml.mlIndexAssetKind\"] = \"acs\"\n",
    "pipeline.properties[\"azureml.mlIndexAssetSource\"] = \"raw_papers\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFileJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Submit the pipeline\n",
    "run = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    experiment_name=\"document-processin-aml-components-pipeline\",\n",
    "    tags={\"type\": \"document-processing\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ml_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mazureml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlindex\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MLIndex\n\u001b[1;32m      5\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow many steps are in metalloporphyrins synthesis?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m faias_retriever \u001b[38;5;241m=\u001b[39m MLIndex(\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mml_client\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mget(asset_name, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m )\u001b[38;5;241m.\u001b[39mas_langchain_retriever()\n\u001b[1;32m     10\u001b[0m retriever\u001b[38;5;241m.\u001b[39mget_relevant_documents(question)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ml_client' is not defined"
     ]
    }
   ],
   "source": [
    "from azureml.rag.mlindex import MLIndex\n",
    "\n",
    "\n",
    "\n",
    "question = \"how many steps are in metalloporphyrins synthesis?\"\n",
    "\n",
    "faias_retriever = MLIndex(\n",
    "    ml_client.data.get(asset_name, label=\"latest\")\n",
    ").as_langchain_retriever()\n",
    "retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.rag.utils.connections import get_connection_by_id_v2\n",
    "\n",
    "aoai_connection_id = ml_client.connections.get(\"aoai-sweden-505\").id\n",
    "aoai_connection = get_connection_by_id_v2(aoai_connection_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/nougat/lib/python3.9/site-packages/azureml/rag/models.py:276: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import AzureChatOpenAI``.\n",
      "  llm = AzureChatOpenAI(\n",
      "/tmp/ipykernel_213310/3078494376.py:22: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  qa.run(question)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know the answer to how many steps are in metalloporph\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from azureml.rag.models import init_llm, parse_model_uri\n",
    "\n",
    "model_config = parse_model_uri(\n",
    "    \"azure_open_ai://deployment/gpt-35-turbo/model/gpt-35-turbo\"\n",
    ")\n",
    "model_config[\"azure_endpoint\"] = aoai_connection.target\n",
    "model_config[\"api_key\"] = aoai_connection.api_key\n",
    "model_config[\"temperature\"] = 0.3\n",
    "model_config[\"max_retries\"] = 3\n",
    "model_config[\"deployment\"] = \"gpt-4v\"\n",
    "model_config[\"model\"] = \"gpt-4\"\n",
    "\n",
    "llm=init_llm(model_config)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "qa.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source_doc_id': 'Eur J Org Chem - 2020 - Breugst - ‐Hole Interactions in Catalysis.pdf', 'chunk_hash': '3116fd17b82e85c53f20791851eea6d233b0edc9d0935a70b463e76335f81c51', 'mtime': None, 'page_number': 15, 'stats': {'tiktokens': 9, 'chars': 22, 'lines': 1}, 'source': {'filename': 'Eur J Org Chem - 2020 - Breugst - ‐Hole Interactions in Catalysis.pdf', 'url': '', 'mtime': None, 'role': nan, 'image_path': nan, 'confidence': nan, 'source': 'azure_document_intelligence', 'bounding_box': '(4.20, 7.24, 5.24, 7.36)', 'page': 15, 'id': '24fdc787-653a-4770-959f-f13ff2a62693'}}, page_content='Received: May 13, 2020'),\n",
       " Document(metadata={'source_doc_id': 'ChemBioChem - 2020 - Valverde - Molecular Recognition in C‐Type Lectins  The Cases of DC‐SIGN  Langerin  MGL  and L‐Sectin.pdf', 'chunk_hash': 'ef66b46c575b5eaadb34e2e9285b1a1e5a5e2ef20559dcad04b62ecb35b997e7', 'mtime': None, 'page_number': 25, 'stats': {'tiktokens': 62, 'chars': 124, 'lines': 1}, 'source': {'filename': 'ChemBioChem - 2020 - Valverde - Molecular Recognition in C‐Type Lectins  The Cases of DC‐SIGN  Langerin  MGL  and L‐Sectin.pdf', 'url': '', 'mtime': None, 'role': nan, 'image_path': nan, 'confidence': nan, 'source': 'azure_document_intelligence', 'bounding_box': '(4.22, 7.86, 7.61, 8.10)', 'page': 25, 'id': 'f5b63d55-2a7c-4f71-a97d-4879be79c6a7'}}, page_content='[183] H. Feinberg, T. J. Rowntree, S. L. Tan, K. Drickamer, W. I. Weis, M. E. Taylor, J. Biol. Chem. 2013, 288, 36762-36771.'),\n",
       " Document(metadata={'source_doc_id': '1-s2.0-S0927796X2030053X-am.pdf', 'chunk_hash': '3ed08b53b72471017a123dea7fd48639660dc726f693bb24b7ef341364da1b45', 'mtime': None, 'page_number': 28, 'stats': {'tiktokens': 74, 'chars': 200, 'lines': 1}, 'source': {'filename': '1-s2.0-S0927796X2030053X-am.pdf', 'url': '', 'mtime': None, 'role': nan, 'image_path': nan, 'confidence': nan, 'source': 'azure_document_intelligence', 'bounding_box': '(0.89, 4.99, 7.37, 5.26)', 'page': 28, 'id': '85054442-d9ff-4ea4-b03a-4884424bd2d2'}}, page_content='[172] M. M. Noack, K. G. Yager, M. Fukuto, G. S. Doerk, R. Li, J. A. Sethian, A kriging-based approach to autonomous experimentation with applications to x-ray scattering, Sci. Rep. 9 (1) (2019) 1-19.'),\n",
       " Document(metadata={'source_doc_id': '2412.20995v1.pdf', 'chunk_hash': '46491784326478e7a78b8370713be65ebb05ea853276b6f79b2fa006930b0b0d', 'mtime': None, 'page_number': 23, 'stats': {'tiktokens': 17, 'chars': 71, 'lines': 1}, 'source': {'filename': '2412.20995v1.pdf', 'url': '', 'mtime': None, 'role': nan, 'image_path': '2412.20995v1/elements/picture/page_23_Picture_23035456701632.png', 'confidence': 0.3221456110477447, 'source': 'layout_detector', 'bounding_box': '(438.29, 227.81, 601.50, 307.09)', 'page': 23, 'id': '2a473ebc-3ea7-49b9-8165-00c61785d8a9'}}, page_content='The image shows the word \"Reasoning\" in red font on a white background.'),\n",
       " Document(metadata={'source_doc_id': 'ChemBioChem-2020PorphyrinsColorimetricPhotometricBiosensorsModernBioanalyticalSystems.pdf', 'chunk_hash': 'e76b4acff71c007fec800d80d7be09442b0139db06af4b54bc29f602ced0f28b', 'mtime': None, 'page_number': 15, 'stats': {'tiktokens': 35, 'chars': 121, 'lines': 1}, 'source': {'filename': 'ChemBioChem-2020PorphyrinsColorimetricPhotometricBiosensorsModernBioanalyticalSystems.pdf', 'url': '', 'mtime': None, 'role': nan, 'image_path': nan, 'confidence': nan, 'source': 'azure_document_intelligence', 'bounding_box': '(4.21, 1.90, 6.15, 2.27)', 'page': 15, 'id': '47d62392-30f8-4143-86d5-119a9ca67a40'}}, page_content='Manuscript received: February 1, 2020 Revised manuscript received: March 4, 2020 Version of record online: March 30, 2020')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_from_llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever_from_llm\n",
    ")\n",
    "\n",
    "qa.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nougat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
