{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify this scientific image into one of the following categories:\n",
    "\n",
    "TABLES/CHARTS:\n",
    "- Data table\n",
    "- Statistical table\n",
    "- Comparison table\n",
    "\n",
    "GRAPHS/PLOTS:\n",
    "- Line graph\n",
    "- Bar chart\n",
    "- Scatter plot\n",
    "- Box plot\n",
    "- Histogram\n",
    "- Pie chart\n",
    "- Heat map\n",
    "- Network graph\n",
    "- Time series\n",
    "\n",
    "MICROSCOPY:\n",
    "- Light microscopy\n",
    "- Electron microscopy\n",
    "- Fluorescence microscopy\n",
    "- Confocal microscopy\n",
    "- Super-resolution microscopy\n",
    "\n",
    "SPECTROSCOPY:\n",
    "- Mass spectroscopy\n",
    "- NMR spectroscopy\n",
    "- IR spectroscopy\n",
    "- UV-vis spectroscopy\n",
    "- X-ray spectroscopy\n",
    "\n",
    "MEDICAL/BIOLOGICAL IMAGING:\n",
    "- X-ray\n",
    "- CT scan\n",
    "- MRI scan\n",
    "- Ultrasound\n",
    "- PET scan\n",
    "- Histology slide\n",
    "- Western blot\n",
    "- Gel electrophoresis\n",
    "\n",
    "DIAGRAMS/ILLUSTRATIONS:\n",
    "- Chemical structure\n",
    "- Molecular diagram\n",
    "- Anatomical illustration\n",
    "- Flowchart\n",
    "- Schematic diagram\n",
    "- Circuit diagram\n",
    "- Mechanical diagram\n",
    "- Process flow diagram\n",
    "\n",
    "MAPS/GEOGRAPHICAL:\n",
    "- Geographic map\n",
    "- GIS visualization\n",
    "- Satellite image\n",
    "- Terrain model\n",
    "\n",
    "MATHEMATICAL:\n",
    "- Equation\n",
    "- Mathematical model\n",
    "- Geometric figure\n",
    "- Mathematical plot\n",
    "\n",
    "COMPUTER-GENERATED:\n",
    "- 3D rendering\n",
    "- Simulation visualization\n",
    "- Computer model\n",
    "\n",
    "MISCELLANEOUS:\n",
    "- Field photograph\n",
    "- Sample photograph\n",
    "- Experimental setup\n",
    "- Equipment photograph\n",
    "- Screenshot\n",
    "- Logo/Institutional insignia\n",
    "- Cover art\n",
    "- Author photograph\n",
    "- Infographic\n",
    "- Conceptual illustration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_CLASSIFICATION_SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert image classifier specializing in scientific literature. Your task is to classify images from academic papers and research documents into precise numbered categories. Analyze each image carefully, considering its visual characteristics, content, and typical usage in scientific literature.\n",
    "\n",
    "Return your analysis as a valid JSON object with exactly this format:\n",
    "{\n",
    "  \"class\": int,\n",
    "  \"confidence\": float\n",
    "}\n",
    "\n",
    "Where \"class\" is the integer corresponding to the image category, and \"confidence\" is a number between 0.0 and 1.0 representing your confidence in the classification. Do not include any other fields in your response - only these two fields in this exact format.\n",
    "\"\"\"\n",
    "IMAGE_CLASSIFICATION_USER_PROMPT = \"\"\"\n",
    "Classify this scientific image into exactly one of the following numbered categories:\n",
    "\n",
    "1. Data table\n",
    "2. Statistical table\n",
    "3. Line graph\n",
    "4. Bar chart\n",
    "5. Scatter plot\n",
    "6. Box plot\n",
    "7. Histogram\n",
    "8. Pie chart\n",
    "9. Heat map\n",
    "10. Network graph\n",
    "11. Time series plot\n",
    "12. Light microscopy\n",
    "13. Electron microscopy\n",
    "14. Fluorescence microscopy\n",
    "15. Confocal microscopy\n",
    "16. Mass spectroscopy\n",
    "17. NMR spectroscopy\n",
    "18. IR spectroscopy\n",
    "19. UV-vis spectroscopy\n",
    "20. X-ray spectroscopy\n",
    "21. X-ray (medical)\n",
    "22. CT scan\n",
    "23. MRI scan\n",
    "24. Ultrasound\n",
    "25. PET scan\n",
    "26. Histology slide\n",
    "27. Western blot\n",
    "28. Gel electrophoresis\n",
    "29. Chemical structure\n",
    "30. Molecular diagram\n",
    "31. Anatomical illustration\n",
    "32. Flowchart\n",
    "33. Schematic diagram\n",
    "34. Circuit diagram\n",
    "35. Mechanical diagram\n",
    "36. Process flow diagram\n",
    "37. Geographic map\n",
    "38. GIS visualization\n",
    "39. Satellite image\n",
    "40. Equation/Mathematical expression\n",
    "41. Geometric figure\n",
    "42. 3D rendering\n",
    "43. Computer simulation\n",
    "44. Field photograph\n",
    "45. Sample photograph\n",
    "46. Experimental setup\n",
    "47. Equipment photograph\n",
    "48. Screenshot\n",
    "49. Logo/Institutional insignia\n",
    "50. Infographic\n",
    "51. other\n",
    "\n",
    "Return only a JSON with the class number (integer) and your confidence (float between 0.0 and 1.0) in this exact format:\n",
    "{\n",
    "  \"class\": int,\n",
    "  \"confidence\": float\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI, OpenAI\n",
    "from azureml.rag.utils.connections import get_connection_by_id_v2\n",
    "from azureml.rag.utils.logging import get_logger, safe_mlflow_start_run, track_activity\n",
    "import os\n",
    "\n",
    "logger = get_logger(\"document_analyzer\")\n",
    "\n",
    "def setup_openai_client(connection_id: str, type: str = \"openai\"):\n",
    "    \"\"\"Set up Azure OpenAI client using connection.\"\"\"\n",
    "    try:\n",
    "        # Get connection details\n",
    "        connection = get_connection_by_id_v2(connection_id)\n",
    "\n",
    "        # Safely access endpoint and api_key from dictionary\n",
    "        endpoint = connection.get(\"endpoint\") or connection.get(\"properties\", {}).get(\"metadata\", {}).get(\"endpoint\")\n",
    "        api_key = connection.get(\"api_key\") or connection.get(\"properties\", {}).get(\"credentials\", {}).get(\"keys\", {}).get(\"api_key\")\n",
    "\n",
    "        if not endpoint or not api_key:\n",
    "            raise ValueError(\"Missing endpoint or api_key in connection details.\")\n",
    "\n",
    "        if type == \"openai\":\n",
    "            # Create client for openai\n",
    "            client = OpenAI(\n",
    "                api_key=api_key,\n",
    "                base_url=endpoint\n",
    "            )\n",
    "        else:                \n",
    "            # Create client for  vision\n",
    "            client = AzureOpenAI(\n",
    "                api_key=api_key,\n",
    "                api_version=\"2025-01-01-preview\",\n",
    "                azure_endpoint=endpoint\n",
    "            )\n",
    "\n",
    "        return client\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to setup Azure OpenAI connection: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "# Example of registering the component in a workspace\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Get workspace\n",
    "ml_client = MLClient.from_config(\n",
    "    credential=DefaultAzureCredential()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aoai_connection_name = \"open_ai_connection\"\n",
    "aoai_connection_name = \"deepseek\"\n",
    "acs_connection_name = \"acs-connection\"\n",
    "data_set_name = \"papers\"\n",
    "asset_name = \"aoai_acs_mlindex\"\n",
    "doc_intelligence_connection_name = \"doc-intelligence-connection\"\n",
    "vision_deploy_name = \"gpt-4\"\n",
    "aoai_embedding_model_name = \"text-embedding-3-large\"\n",
    "\n",
    "acs_config = {\n",
    "    \"index_name\": \"qknows-embedding\",\n",
    "}\n",
    "\n",
    "experiment_name = \"sample-acs-embedding\"\n",
    "\n",
    "aoai_connection_id = ml_client.connections.get(aoai_connection_name).id\n",
    "\n",
    "aoai_cleint = setup_openai_client(aoai_connection_id)\n",
    "\n",
    "# Define your deployment name for the vision model (e.g., \"gpt-4v\")\n",
    "vision_deployment_name = \"gpt-4v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://deepseek-r1-ilhid.westus3.models.ai.azure.com'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = aoai_cleint.base_url\n",
    "str(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='a70d405895644827962dde9033e4acf0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='<think>\\nOkay, the user said \"Say this is a test\". Let me make sure I understand what they\\'re asking for. They probably want me to respond by repeating the phrase \"this is a test\". Maybe they\\'re checking if the system is working or just trying out how the bot responds. Let me keep it simple and just say exactly that. Sometimes users want a straightforward reply without any extra information. I\\'ll go with \"This is a test.\" as the response. That should cover it. If they need more, they\\'ll ask again.\\n</think>\\n\\nThis is a test.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, reasoning_content=None))], created=1741116879, model='deepseek-r1', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=120, prompt_tokens=8, total_tokens=128, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "chat_completion = aoai_cleint.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"deepseek-r1\",\n",
    ")\n",
    "\n",
    "print(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user said \"Say this is a test\". Let me make sure I understand what they're asking for. They probably want me to respond by repeating the phrase \"this is a test\". Maybe they're checking if the system is working or just trying out how the bot responds. Let me keep it simple and just say exactly that. Sometimes users want a straightforward reply without any extra information. I'll go with \"This is a test.\" as the response. That should cover it. If they need more, they'll ask again.\n",
      "</think>\n",
      "\n",
      "This is a test.\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: deepseek-r1\n",
      "Model type: chat-completion\n",
      "Model provider name: DeepSeek\n",
      "Response: <think>\n",
      "Okay, the user asked, \"What is so great about #1?\" referring to the Eiffel Tower from my previous list. I need to elaborate on why the Eiffel Tower is a must-see. First, I should consider their context: they're planning a trip to Paris and looking for highlights. They probably want to know specifics that make it stand out. \n",
      "\n",
      "They might be curious about its history, unique features, or experiences offered. Maybe they're wondering if it's worth visiting despite being so touristy. I should highlight its historical significance, architectural marvel, the views, and maybe some tips like visiting at different times or dining options. Also, mention its cultural impact as a symbol of Paris and romance.\n",
      "\n",
      "Should I structure the answer with clear sections? Maybe bullet points again, but in a more detailed way. Also, check facts: when was it built, who designed it, any interesting facts like initial criticism. Include practical info like the three levels, restaurants, light shows. Emphasize how it's more than just a tower but an iconic symbol. Avoid jargon, keep it engaging. Make sure to answer why it's \"great\" beyond just being famous. Connect it to the user's potential interests—views for photographers, history for enthusiasts, romance for couples. Alright, let's put this together clearly and enthusiastically.\n",
      "</think>\n",
      "\n",
      "The **Eiffel Tower** (or *Tour Eiffel*) is more than just an iconic landmark—it’s a symbol of Paris and a marvel of human ingenuity. Here’s why it’s so special:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Historical Significance**  \n",
      "- Built in **1889** for the *World’s Fair* to celebrate the 100th anniversary of the French Revolution.  \n",
      "- Initially criticized by artists and intellectuals as an eyesore, it later became a beloved global icon.  \n",
      "- Designed by **Gustave Eiffel**, whose company also worked on the Statue of Liberty’s framework.  \n",
      "\n",
      "---\n",
      "\n",
      "### **2. Architectural Wonder**  \n",
      "- **Height**: At 330 meters (1,083 feet), it was the tallest human-made structure in the world until 1930.  \n",
      "- **Engineering**: A lattice iron structure with 18,038 metal parts and 2.5 million rivets—revolutionary for its time.  \n",
      "- **Adaptability**: Built to sway slightly in strong winds and expand under heat, ensuring durability.  \n",
      "\n",
      "---\n",
      "\n",
      "### **3. Breathtaking Views**  \n",
      "- **Three Levels**:  \n",
      "  - **1st floor**: Glass floors, exhibits, and a champagne bar.  \n",
      "  - **2nd floor**: Panoramic views of Paris (ideal for photos of landmarks like the Louvre or Sacré-Cœur).  \n",
      "  - **Summit**: A dizzying 276 meters up, with a historic office of Gustave Eiffel and telescopes.  \n",
      "- **Nighttime Sparkle**: Every evening, the tower is illuminated by 20,000 golden lights and \"sparkles\" for 5 minutes hourly until 1 a.m.  \n",
      "\n",
      "---\n",
      "\n",
      "### **4. Cultural Impact**  \n",
      "- A **universal symbol** of romance, adventure, and Paris itself, featured in countless films, books, and art.  \n",
      "- Hosts unique experiences: Fine dining at **Le Jules Verne** (Michelin-starred restaurant), seasonal ice rinks, and light shows.  \n",
      "\n",
      "---\n",
      "\n",
      "### **5. Emotional Experience**  \n",
      "- Walking beneath its massive iron arches or picnicking on the **Champ de Mars** at sunset is unforgettable.  \n",
      "- Climbing the stairs (or riding the elevators) gives a tangible connection to history.  \n",
      "\n",
      "---\n",
      "\n",
      "### **Fun Fact**:  \n",
      "The tower was almost torn down in 1909! It was saved because its antenna became crucial for early radio transmissions.  \n",
      "\n",
      "Whether you admire it from afar or ascend to its peak, the Eiffel Tower embodies the magic of Paris. It’s a must-see to understand why the city is called the \"City of Light.\" ✨\n",
      "Model: deepseek-r1\n",
      "Usage:\n",
      "\tPrompt tokens: 200\n",
      "\tTotal tokens: 1031\n",
      "\tCompletion tokens: 831\n"
     ]
    }
   ],
   "source": [
    "# pip install azure-ai-inference\n",
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "connection = get_connection_by_id_v2(aoai_connection_id)\n",
    "api_key = connection.get(\"properties\", {}).get(\"credentials\", {}).get(\"keys\", {}).get(\"api_key\")\n",
    "if not api_key:\n",
    "  raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint='https://DeepSeek-R1-ilhid.westus3.models.ai.azure.com',\n",
    "    credential=AzureKeyCredential(api_key)\n",
    ")\n",
    "\n",
    "model_info = client.get_model_info()\n",
    "print(\"Model name:\", model_info.model_name)\n",
    "print(\"Model type:\", model_info.model_type)\n",
    "print(\"Model provider name:\", model_info.model_provider_name)\n",
    "\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"I am going to Paris, what should I see?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What is so great about #1?\"\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 2048\n",
    "}\n",
    "response = client.complete(payload)\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content)\n",
    "print(\"Model:\", response.model)\n",
    "print(\"Usage:\")\n",
    "print(\"\tPrompt tokens:\", response.usage.prompt_tokens)\n",
    "print(\"\tTotal tokens:\", response.usage.total_tokens)\n",
    "print(\"\tCompletion tokens:\", response.usage.completion_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassification(DocumentProcessor):\n",
    "    def __init__(self, input_folder, output_folder, openai_client, vision_deployment_name):\n",
    "        super().__init__(input_folder, output_folder, openai_client, vision_deployment_name)\n",
    "        self.vision_client = openai_client\n",
    "        self.vision_deployment_name = vision_deployment_name\n",
    "    \n",
    "    def get_image_classification(self, image_path):\n",
    "        try:\n",
    "            base64_image = self.encode_image(image_path)\n",
    "            if not base64_image:\n",
    "                print(f\"Skipping classification for invalid image: {image_path}\")\n",
    "                return None\n",
    "            \n",
    "            max_retries = 3\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    # Method 1: Try with response_format as JSON object\n",
    "                    # This is the format for newer API versions\n",
    "                    try:\n",
    "                        response = self.vision_client.chat.completions.create(\n",
    "                            model=self.vision_deployment_name,\n",
    "                            response_format={\"type\": \"json_object\"},\n",
    "                            messages=[\n",
    "                                {\n",
    "                                    \"role\": \"system\",\n",
    "                                    \"content\": IMAGE_CLASSIFICATION_SYSTEM_PROMPT\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": [\n",
    "                                        {\"type\": \"text\", \"text\": IMAGE_CLASSIFICATION_USER_PROMPT},\n",
    "                                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "                                    ]\n",
    "                                }\n",
    "                            ],\n",
    "                            temperature=0.0,\n",
    "                            max_tokens=100\n",
    "                        )\n",
    "                        return json.loads(response.choices[0].message.content)\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        if \"extra fields not permitted\" in str(e):\n",
    "                            # Method 2: Try without response_format parameter\n",
    "                            # This is for older API versions that don't support response_format\n",
    "                            response = self.vision_client.chat.completions.create(\n",
    "                                model=self.vision_deployment_name,\n",
    "                                messages=[\n",
    "                                    {\n",
    "                                        \"role\": \"system\",\n",
    "                                        \"content\": IMAGE_CLASSIFICATION_SYSTEM_PROMPT + \"\\nYour response must be valid JSON.\"\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"role\": \"user\",\n",
    "                                        \"content\": [\n",
    "                                            {\"type\": \"text\", \"text\": IMAGE_CLASSIFICATION_USER_PROMPT},\n",
    "                                            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "                                        ]\n",
    "                                    }\n",
    "                                ],\n",
    "                                temperature=0.0,\n",
    "                                max_tokens=100\n",
    "                            )\n",
    "                            return json.loads(response.choices[0].message.content)\n",
    "                        else:\n",
    "                            raise e\n",
    "                        \n",
    "                except json.JSONDecodeError:\n",
    "                    # If the response isn't valid JSON, try to extract JSON from the text\n",
    "                    content = response.choices[0].message.content\n",
    "                    try:\n",
    "                        # Try to find JSON within the response\n",
    "                        json_start = content.find('{')\n",
    "                        json_end = content.rfind('}') + 1\n",
    "                        if json_start >= 0 and json_end > json_start:\n",
    "                            json_str = content[json_start:json_end]\n",
    "                            return json.loads(json_str)\n",
    "                        else:\n",
    "                            print(f\"Could not extract JSON from response: {content}\")\n",
    "                            return None\n",
    "                    except Exception as json_extract_err:\n",
    "                        print(f\"Failed to parse JSON response: {str(json_extract_err)}\")\n",
    "                        return None\n",
    "                \n",
    "                except Exception as e:\n",
    "                    if attempt < max_retries - 1:\n",
    "                        print(f\"Image classification attempt {attempt + 1} failed: {str(e)}\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(f\"All image classification attempts failed for {image_path}\")\n",
    "                        return None\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Image classification failed: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image classified as: Flowchart (ID: 32)\n",
      "Confidence: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Initialize the image classification service\n",
    "classifier = ImageClassification(\n",
    "    input_folder= \"/home/azureuser/academic-document-analyzer/docs/\",\n",
    "    output_folder= \".\",\n",
    "    openai_client=aoai_cleint,\n",
    "    vision_deployment_name=vision_deployment_name\n",
    ")\n",
    "\n",
    "# Path to the image you want to classify\n",
    "image_path = \"/home/azureuser/academic-document-analyzer/docs/indexer_pipeline-next-step.png\"\n",
    "\n",
    "# Classify the image\n",
    "result = classifier.get_image_classification(image_path)\n",
    "\n",
    "# Process the result\n",
    "if result:\n",
    "    class_id = result[\"class\"]\n",
    "    confidence = result[\"confidence\"]\n",
    "    \n",
    "    # Map class ID to class name (you would have this mapping defined somewhere)\n",
    "    class_names = {\n",
    "        1: \"Data table\",\n",
    "        2: \"Statistical table\",\n",
    "        3: \"Line graph\",\n",
    "        4: \"Bar chart\",\n",
    "        5: \"Scatter plot\",\n",
    "        6: \"Box plot\",\n",
    "        7: \"Histogram\",\n",
    "        8: \"Pie chart\",\n",
    "        9: \"Heat map\",\n",
    "        10: \"Network graph\",                \n",
    "        11: \"Time series plot\",\n",
    "        12: \"Light microscopy\",\n",
    "        13: \"Electron microscopy\",\n",
    "        14: \"Fluorescence microscopy\",\n",
    "        15: \"Confocal microscopy\",\n",
    "        16: \"Mass spectroscopy\",\n",
    "        17: \"NMR spectroscopy\",\n",
    "        18: \"IR spectroscopy\",\n",
    "        19: \"UV-vis spectroscopy\",\n",
    "        20: \"X-ray spectroscopy\",\n",
    "        21: \"X-ray (medical)\",\n",
    "        22: \"CT scan\",\n",
    "        23: \"MRI scan\",\n",
    "        24: \"Ultrasound\",\n",
    "        25: \"PET scan\",\n",
    "        26: \"Histology slide\",\n",
    "        27: \"Western blot\",\n",
    "        28: \"Gel electrophoresis\",\n",
    "        29: \"Chemical structure\",\n",
    "        30: \"Molecular diagram\",\n",
    "        31: \"Anatomical illustration\",\n",
    "        32: \"Flowchart\",\n",
    "        33: \"Schematic diagram\",\n",
    "        34: \"Circuit diagram\",\n",
    "        35: \"Mechanical diagram\",\n",
    "        36: \"Process flow diagram\",\n",
    "        37: \"Geographic map\",\n",
    "        38: \"GIS visualization\",    \n",
    "        39: \"Satellite image\",\n",
    "        40: \"Equation/Mathematical expression\",\n",
    "        41: \"Geometric figure\",\n",
    "        42: \"3D rendering\",     \n",
    "        43: \"Computer simulation\",\n",
    "        44: \"Field photograph\",\n",
    "        45: \"Sample photograph\",\n",
    "        46: \"Experimental setup\",\n",
    "        47: \"Equipment photograph\",\n",
    "        48: \"Screenshot\",\n",
    "        49: \"Logo/Institutional insignia\",\n",
    "        50: \"Infographic\",\n",
    "        51: \"other\"\n",
    "    }\n",
    "    \n",
    "    class_name = class_names.get(class_id, \"Unknown class\")\n",
    "    \n",
    "    print(f\"Image classified as: {class_name} (ID: {class_id})\")\n",
    "    print(f\"Confidence: {confidence:.2f}\")\n",
    "else:\n",
    "    print(\"Classification failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
